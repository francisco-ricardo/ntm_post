\documentclass[a4paper]{article}

\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}

\usepackage{listings}

\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}


\title{
    Machine Translation: Brief Overview\\
}

\author{
    \small{Author: Francisco Ricardo Taborda Aguiar}\\
}

\date{\today}

\begin{document}

    \maketitle

    Machine Translation (MT) is an important research field within 
    Natural Language Processing (NLP). MT involves the use of Machine 
    Learning techniques to translate text or speech from a source 
    language to a target language. The concept of using computers 
    to translate natural languages dates back to the 1950s. 
    However, it wasn't until nearly 50 years later that hardware, 
    data, and software became advanced enough to perform the basics 
    of machine translation. Over time, the MT process has evolved, 
    becoming faster and more precise. Today, many different MT 
    strategies exist to facilitate accurate and efficient translation.

    A few years back, MT systems were developed using Statistical Machine 
    Translation (SMT) techniques, which could extract implicit information 
    from \emph{corpora bil\'ingues} \cite{brown:1993}. However, the authors 
    of \cite{maruf:2021} found that the SMT features were confined to the 
    technologies used, making the approach rigid.

    Many studies have shown an increase in the use of neural networks in 
    NLP, as mentioned by \cite{goldberg:2016}. However, most of these 
    approaches involve incorporating neural networks as components into 
    traditional systems, replacing only certain parts of the existing 
    architecture, as confirmed by \cite{stahlberg:2020}.

    MT has made significant progress with the development of NMT systems. 
    These systems are built on Neural Networks that translate sentences. 
    NMT has become the leading approach for the research and development 
    of MT systems \cite{tan:2020}. NMT systems have also been implemented 
    in production systems, such as Google \cite{wu:2016}.

    The application of Deep Neural Networks (DNN) in MT has gained 
    significant attention through models known as Sequence-to-Sequence
    \cite{gehring:2017}, which utilize Recurrent Neural Networks (RNN). 
    These models have a fundamental architecture that includes an RNN 
    encoder that goes through each token of the source text and generates 
    a state vector of a fixed length. Subsequently, an RNN decoder produces 
    the target sentence one token at a time, using the state vector 
    generated by the encoder, as explained in more detail by 
    \cite{sutskever:2014}.    

    The Transformer is another type of architecture used in NMT systems. 
    Unlike other models, it does not require recurrency or convolution. 
    Instead, it uses a mechanism called Attention 
    \cite{vaswani:2017attention}, which works by 
    mapping questions and sets of key-value pairs to an output. 
    Each of these elements corresponds to vectors. To calculate the 
    output, the model uses a weighted sum of the values, where each 
    value is assigned a weight based on a function of compatibility 
    between the question and the corresponding key.

    \section*{Conclusion}

    MT refers to the use of computers to translate natural languages. 
    This concept was first introduced in the 1950s when computers 
    lacked the processing power to handle data. 
    However, technology has advanced significantly since then, and 
    translation models have evolved from statistical databases to 
    neural networks.
    NMT has emerged as the dominant method in the development of MT 
    systems and has generated interest in various research fields.


\bibliographystyle{ieeetr}
\bibliography{article} 

\end{document}


    At\'e h\'a alguns anos o desenvolvimento de sistemas de tradu\c c\~ao autom\'atica 
    (Machine Translation, ou MT) era, em sua maioria, implementado atrav\'es de t\'ecnicas 
    estat\'isticas, conhecidas como Tradutor de M\'aquinas Estat\'isticas 
    (Statistical Machine Translation, ou SMT).
    Estas t\'ecnicas tornaram capaz a extra\c c\~ao de informa\c c\~oes impl\'icitas a partir de
    \emph{copora bil\'ingues}, \cite{brown:1993}. Por\'em, em \cite{maruf:2021}, verifica-se que 
    as caracter\'isticas do \emph{SMT} eram intr\'insecas \`a tecnologia e isto tornava estas 
    t\'ecnicas inflex\'iveis. V\'arios trabalhos t\^em impulsionado o emprego de redes 
    neurais no Processamento de Linguagem Natural (NLP), conforme pode ser verificado em 
    \cite{goldberg:2016}. Por\'em, a maioria das abordagens consistiam em utilizar redes neurais 
    como sendo componentes em sistemas \emph{STM} tradicionais, apenas substuindo algumas 
    partes da arquitetura, como pode ser notado em \cite{stahlberg:2020}.

    O processo de \emph{MT} tem avan\c cado para o uso de sistemas conhecidos como Tradu\c c\~ao 
    Autom\'atica Neural (Neural Machine Translation, ou NMT).
    Estes sistemas s\~ao baseados em redes neurais que fazem a tradu\c c\~ao das senten\c cas.
    \emph{NMT} vem se tornando o paradigma dominante para a pesquisa e o desenvolvimento 
    de sistemas de \emph{MT} e tem sido utilizado em sistemas de produ\c c\~ao, como na Google
    \cite{wu:2016}, por exemplo.    

    O uso de Redes Neurais Profundas (Deep Neural Network, ou DNN) no processo de \emph{MT}
    tem se destacado atrav\'es dos modelos Sequ\^encia a Sequ\^encia, que fazem o uso de 
    Redes Neurais Recorrentes (Recurrent Neural Networks, ou RNNs).
    A arquitetura b\'asica destes modelos consiste em um codificador RNN que percorre cada 
    token da senten\c ca de origem e gera um vetor de estado com tamanho fixo.
    Na sequ\^encia, um decodificador RNN gera a senten\c ca de destino, um token de cada vez,
    a partir do vetor de estado, como pode ser visto em \cite{sutskever:2014}.
    \cite{kalchbrenner:2016} utilizaram uma Rede Neural Convolucional (Convolutional 
    Neural Network, ou CNN). O codificador e o decodificador s\~ao conectados e as sequ\^encias 
    temporais s\~ao preservadas. Os autores introduziram um mecanismo para tratar das 
    diferen\c cas entre o comprimento da origem e o comprimento do destino.


    An approach used to NLP processing consists of representing the words 
    or sentences as continuous vector. This technique is used in language 
    modeling to transform a word $x \in \sum$ in a vector of dimension 
    $d$ of real numbers to be processed in subsequent layers of neural network.
    The word mapping can be represented by a matrix  
    $E \in \mathbb{R} ^{d \times |\sum|}$, as verified by \cite{collobert:2008} 
    and \cite{stahlberg:2020}.
    According to \cite{stahlberg:2020} the learning from words representation 
    has the potential to capture morphological, syntactic and semantic 
    similarity between words.    