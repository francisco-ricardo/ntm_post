\documentclass[a4paper]{article}

\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}

\usepackage{listings}

\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}


\title{
    Machine Translation\\
}

\author{
    \small{Author: Francisco Ricardo Taborda Aguiar}\\
}

\date{\today}

\begin{document}

    \maketitle

    Until a few years ago the development of Machine Translation (MT) 
    systems was mostly implemented by statistical techniques, known as 
    Statistical Machine Translation (SMT).
    Those techniques became able to extract implicit information from
    \emph{copora bil\'ingues} , \cite{brown:1993}.
    However, the authors of \cite{maruf:2021} noted that the SMT features
    were intrinsic to the used technologies and this made the approach
    inflexible. 
    
    Several works have boosted the employment of neural 
    networks on Natural Language Processing (NLP), as noted by 
    \cite{goldberg:2016}.
    However, the majority of the approachs consist of using Neural Networks
    as components in traditonal systems, only replacing some parts of the
    architecture, as verified by \cite{stahlberg:2020}.

    An approach used to NLP processing consists of representing the words 
    or sentences as continuous vector. This technique is used in language 
    modeling to transform a word $x \in \sum$ in a vector of dimension 
    $d$ of real numbers to be processed in subsequent layers of neural network.
    The word mapping can be represented by a matrix  
    $E \in \mathbb{R} ^{d \times |\sum|}$, as verified by \cite{collobert:2008} 
    and \cite{stahlberg:2020}.
    According to \cite{stahlberg:2020} the learning from words representation 
    has the potential to capture morphological, syntactic and semantic 
    similarity between words.    

    MT has advanced to the use of systems known as 
    Neural Machine Translation (NMT). These systems are based on Neural 
    Networks which translate the sentences.
    NMT has advanced to become the dominant paradigm for research and 
    development of  MT systems and has been used in production systems,
    like Google \cite{wu:2016}, for example.

    The use of Deep Neural Networks (DNN) on MT has stood out through 
    models called Sequence-to-Sequence, which use Recurrent Neural Networks
    (RNN). The basic architecture of these models consists of a RNN encoder
    that cycle through each token of the source origin and generates a 
    state vector of fixed length.
    Next, a RNN decoder generates the target sentence, one token each time,
    from the state vector, as explained in more detail by \cite{sutskever:2014}.

    Another architecture used in NMT systems is the Transformer, introduced 
    by \cite{vaswani:2017attention}. This model uses a mechanism called attention 
    and does not require the use of recurrency and neither convolution. 
    The mechanism is based on a function which maps questions and a set of 
    key-value pairs on the output.
    The questions, the keys, the values and the output correspond to vectors.
    The output is calculated through a weighted sum of the values, and the 
    weight is associated for each value. The weight is calculated through
    a function of compatibility between the question and the corresponding 
    key.

    TODO       
    
    A Tradução Automática Neural poderia ser aplicada para a conversão dos dialetos RS274-D 
    para as Funções Canônicas de Usinagem. Porém, vale observar que isto demandaria a obtenção de 
    um acervo de programas escritos nos diversos dialetos para vialibilizar os processos de 
    treinamento e teste da rede neural.

\bibliographystyle{ieeetr}
\bibliography{article} 

\end{document}


    At\'e h\'a alguns anos o desenvolvimento de sistemas de tradu\c c\~ao autom\'atica 
    (Machine Translation, ou MT) era, em sua maioria, implementado atrav\'es de t\'ecnicas 
    estat\'isticas, conhecidas como Tradutor de M\'aquinas Estat\'isticas 
    (Statistical Machine Translation, ou SMT).
    Estas t\'ecnicas tornaram capaz a extra\c c\~ao de informa\c c\~oes impl\'icitas a partir de
    \emph{copora bil\'ingues}, \cite{brown:1993}. Por\'em, em \cite{maruf:2021}, verifica-se que 
    as caracter\'isticas do \emph{SMT} eram intr\'insecas \`a tecnologia e isto tornava estas 
    t\'ecnicas inflex\'iveis. V\'arios trabalhos t\^em impulsionado o emprego de redes 
    neurais no Processamento de Linguagem Natural (NLP), conforme pode ser verificado em 
    \cite{goldberg:2016}. Por\'em, a maioria das abordagens consistiam em utilizar redes neurais 
    como sendo componentes em sistemas \emph{STM} tradicionais, apenas substuindo algumas 
    partes da arquitetura, como pode ser notado em \cite{stahlberg:2020}.

    O processo de \emph{MT} tem avan\c cado para o uso de sistemas conhecidos como Tradu\c c\~ao 
    Autom\'atica Neural (Neural Machine Translation, ou NMT).
    Estes sistemas s\~ao baseados em redes neurais que fazem a tradu\c c\~ao das senten\c cas.
    \emph{NMT} vem se tornando o paradigma dominante para a pesquisa e o desenvolvimento 
    de sistemas de \emph{MT} e tem sido utilizado em sistemas de produ\c c\~ao, como na Google
    \cite{wu:2016}, por exemplo.    

    O uso de Redes Neurais Profundas (Deep Neural Network, ou DNN) no processo de \emph{MT}
    tem se destacado atrav\'es dos modelos Sequ\^encia a Sequ\^encia, que fazem o uso de 
    Redes Neurais Recorrentes (Recurrent Neural Networks, ou RNNs).
    A arquitetura b\'asica destes modelos consiste em um codificador RNN que percorre cada 
    token da senten\c ca de origem e gera um vetor de estado com tamanho fixo.
    Na sequ\^encia, um decodificador RNN gera a senten\c ca de destino, um token de cada vez,
    a partir do vetor de estado, como pode ser visto em \cite{sutskever:2014}.
    \cite{kalchbrenner:2016} utilizaram uma Rede Neural Convolucional (Convolutional 
    Neural Network, ou CNN). O codificador e o decodificador s\~ao conectados e as sequ\^encias 
    temporais s\~ao preservadas. Os autores introduziram um mecanismo para tratar das 
    diferen\c cas entre o comprimento da origem e o comprimento do destino.